#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£ç±»å‹åˆ†æå’Œå¯è§†åŒ–

è‡ªåŠ¨ä»WOSæ–‡ä»¶ä¸­æå–æ–‡æ¡£ç±»å‹ç»Ÿè®¡å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""

import pandas as pd
import matplotlib.pyplot as plt
import re
from pathlib import Path
from typing import Dict, Tuple


class DocumentTypeAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self._setup_plot_style()
        self._setup_color_palette()

    def _setup_plot_style(self):
        """è®¾ç½®å›¾è¡¨æ ·å¼"""
        try:
            plt.rcParams['font.family'] = 'Arial'
        except:
            pass
        plt.rcParams['axes.labelsize'] = 14
        plt.rcParams['xtick.labelsize'] = 12
        plt.rcParams['ytick.labelsize'] = 12
        plt.rcParams['legend.fontsize'] = 12
        plt.rcParams['figure.titlesize'] = 22
        plt.rcParams['axes.titlesize'] = 18
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['axes.labelweight'] = 'bold'
        plt.style.use('seaborn-v0_8-whitegrid')

    def _setup_color_palette(self):
        """è®¾ç½®é¢œè‰²"""
        self.palette = {
            'Article': '#1f77b4',
            'Review': '#ff7f0e',
        }

    def parse_wos_file(self, file_path: str, min_year: int = None, max_year: int = None) -> Dict[str, int]:
        """è§£æWOSæ–‡ä»¶ï¼Œç»Ÿè®¡æ–‡æ¡£ç±»å‹

        Args:
            file_path: WOSæ–‡ä»¶è·¯å¾„
            min_year: æœ€å°å¹´ä»½ï¼ˆå¯é€‰ï¼Œç”¨äºç­›é€‰ï¼‰
            max_year: æœ€å¤§å¹´ä»½ï¼ˆå¯é€‰ï¼Œç”¨äºç­›é€‰ï¼‰
        """
        counts = {'Article': 0, 'Review': 0}

        with open(file_path, 'r', encoding='utf-8-sig') as f:
            content = f.read()

        records = content.split('\n\nPT ')[1:]  # è·³è¿‡æ–‡ä»¶å¤´

        for record in records:
            if record.strip():
                match = re.search(r'^J\s*$', record, re.MULTILINE)
                if match:
                    # å¦‚æœæŒ‡å®šäº†å¹´ä»½èŒƒå›´ï¼Œå…ˆæ£€æŸ¥å¹´ä»½
                    if min_year is not None or max_year is not None:
                        py_match = re.search(r'^PY\s+(\d{4})', record, re.MULTILINE)
                        if py_match:
                            year = int(py_match.group(1))
                            # å¹´ä»½ä¸åœ¨èŒƒå›´å†…ï¼Œè·³è¿‡
                            if min_year and year < min_year:
                                continue
                            if max_year and year > max_year:
                                continue
                        else:
                            # æ²¡æœ‰å¹´ä»½ä¿¡æ¯ï¼Œè·³è¿‡
                            continue

                    # æŸ¥æ‰¾DTå­—æ®µ
                    dt_match = re.search(r'^DT\s+(.+?)$', record, re.MULTILINE)
                    if dt_match:
                        doc_type = dt_match.group(1).strip()
                        if 'Article' in doc_type:
                            counts['Article'] += 1
                        elif 'Review' in doc_type:
                            counts['Review'] += 1

        return counts

    def create_data_from_files(self, wos_file: str, scopus_file: str, final_file: str,
                               min_year: int = None, max_year: int = None) -> pd.DataFrame:
        """ä»ä¸‰ä¸ªæ–‡ä»¶ä¸­æå–æ•°æ®

        Args:
            wos_file: WOS æ–‡ä»¶è·¯å¾„
            scopus_file: Scopus è½¬æ¢åæ–‡ä»¶è·¯å¾„
            final_file: æœ€ç»ˆæ–‡ä»¶è·¯å¾„
            min_year: æœ€å°å¹´ä»½ï¼ˆå¯é€‰ï¼‰
            max_year: æœ€å¤§å¹´ä»½ï¼ˆå¯é€‰ï¼‰
        """
        # å¯¹ WOS åº”ç”¨å¹´ä»½ç­›é€‰
        wos_counts = self.parse_wos_file(wos_file, min_year, max_year)
        # Scopus æ–‡ä»¶å·²ç»åœ¨workflowä¸­è¿‡æ»¤è¿‡å¹´ä»½ï¼Œä¸éœ€è¦å†æ¬¡ç­›é€‰
        # ï¼ˆscopus_enriched.txt æ¥è‡ª scopus_year_filtered.csvï¼Œå·²åŒ…å«å¹´ä»½ç­›é€‰ï¼‰
        scopus_counts = self.parse_wos_file(scopus_file, None, None)
        # Final æ–‡ä»¶å·²ç»æ˜¯ç­›é€‰åçš„ï¼Œä¸éœ€è¦å†æ¬¡ç­›é€‰
        final_counts = self.parse_wos_file(final_file)

        data = pd.DataFrame({
            'Article_Type': ['Article', 'Review'],
            'WoS_Count': [wos_counts['Article'], wos_counts['Review']],
            'Scopus_Count': [scopus_counts['Article'], scopus_counts['Review']],
            'Final_Count': [final_counts['Article'], final_counts['Review']]
        })

        return data

    def plot_distribution(self, data: pd.DataFrame, output_dir: str):
        """ç»˜åˆ¶æ–‡æ¡£ç±»å‹åˆ†å¸ƒå›¾"""
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(22, 8))

        donut_width = 0.4
        wedge_props = {'width': donut_width, 'edgecolor': 'white', 'linewidth': 2}
        text_props = {'fontsize': 16, 'fontweight': 'bold'}

        # Web of Science
        data1 = data[['Article_Type', 'WoS_Count']].rename(columns={'WoS_Count': 'Count'})
        total1 = int(data1['Count'].sum())
        if total1 > 0:
            colors1 = [self.palette[cat] for cat in data1['Article_Type']]
            labels1 = [f"{row['Article_Type']}\n(n={int(row['Count'])})" for _, row in data1.iterrows()]
            ax1.pie(data1['Count'], labels=labels1, colors=colors1, autopct='%1.1f%%',
                    startangle=90, wedgeprops=wedge_props, textprops=text_props, pctdistance=0.8)
            ax1.text(0, 0, f'n={total1}', ha='center', va='center', fontsize=30, fontweight='bold')
        else:
            ax1.text(0.5, 0.5, 'No Data', ha='center', va='center', fontsize=20,
                    transform=ax1.transAxes, color='gray')
        ax1.set_title('Web of Science', pad=20)

        # Scopus
        data2 = data[['Article_Type', 'Scopus_Count']].rename(columns={'Scopus_Count': 'Count'})
        total2 = int(data2['Count'].sum())
        if total2 > 0:
            colors2 = [self.palette[cat] for cat in data2['Article_Type']]
            labels2 = [f"{row['Article_Type']}\n(n={int(row['Count'])})" for _, row in data2.iterrows()]
            ax2.pie(data2['Count'], labels=labels2, colors=colors2, autopct='%1.1f%%',
                    startangle=90, wedgeprops=wedge_props, textprops=text_props, pctdistance=0.8)
            ax2.text(0, 0, f'n={total2}', ha='center', va='center', fontsize=30, fontweight='bold')
        else:
            ax2.text(0.5, 0.5, 'No Data', ha='center', va='center', fontsize=20,
                    transform=ax2.transAxes, color='gray')
        ax2.set_title('Scopus', pad=20)

        # Final Dataset
        data3 = data[['Article_Type', 'Final_Count']].rename(columns={'Final_Count': 'Count'})
        total3 = int(data3['Count'].sum())
        if total3 > 0:
            colors3 = [self.palette[cat] for cat in data3['Article_Type']]
            labels3 = [f"{row['Article_Type']}\n(n={int(row['Count'])})" for _, row in data3.iterrows()]
            ax3.pie(data3['Count'], labels=labels3, colors=colors3, autopct='%1.1f%%',
                    startangle=90, wedgeprops=wedge_props, textprops=text_props, pctdistance=0.8)
            ax3.text(0, 0, f'n={total3}', ha='center', va='center', fontsize=30, fontweight='bold')
        else:
            ax3.text(0.5, 0.5, 'No Data', ha='center', va='center', fontsize=20,
                    transform=ax3.transAxes, color='gray')
        ax3.set_title('Final Dataset', pad=20)

        # è®¾ç½®ç™¾åˆ†æ¯”æ ‡ç­¾é¢œè‰²ä¸ºç™½è‰²
        for ax in [ax1, ax2, ax3]:
            for text in ax.texts:
                if text.get_text().endswith('%'):
                    text.set_color('white')
                    text.set_fontsize(16)
                    text.set_fontweight('bold')

        fig.suptitle('Distribution of Articles and Reviews Across Databases', y=1.02, fontweight='bold')
        fig.tight_layout(rect=[0, 0, 1, 0.95])

        # ä¿å­˜å›¾ç‰‡
        output_path = Path(output_dir)
        for fmt in ['tiff', 'png']:
            fig.savefig(output_path / f'document_types.{fmt}', dpi=300, bbox_inches='tight',
                       format=fmt, facecolor='white', edgecolor='none')

        plt.close(fig)
        print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {output_path}/document_types.tiff å’Œ .png")


def generate_document_type_analysis(data_dir: str, min_year: int = None, max_year: int = None):
    """ç”Ÿæˆæ–‡æ¡£ç±»å‹åˆ†æ

    Args:
        data_dir: æ•°æ®ç›®å½•
        min_year: æœ€å°å¹´ä»½ï¼ˆå¯é€‰ï¼Œå¦‚æœæŒ‡å®šåˆ™å¯¹ WOS å’Œ Scopus æ•°æ®ä¹Ÿè¿›è¡Œå¹´ä»½ç­›é€‰ï¼‰
        max_year: æœ€å¤§å¹´ä»½ï¼ˆå¯é€‰ï¼Œå¦‚æœæŒ‡å®šåˆ™å¯¹ WOS å’Œ Scopus æ•°æ®ä¹Ÿè¿›è¡Œå¹´ä»½ç­›é€‰ï¼‰
    """
    data_dir = Path(data_dir)

    # æ–‡ä»¶è·¯å¾„ï¼ˆv4.5.0æ›´æ–°ï¼šå¹´ä»½è¿‡æ»¤å·²åœ¨æºå¤´å®Œæˆï¼‰
    wos_file = data_dir / 'wos.txt'

    # Scopusæ–‡ä»¶ï¼šä¼˜å…ˆä½¿ç”¨AIè¡¥å…¨åçš„æ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨è½¬æ¢åçš„æ–‡ä»¶
    scopus_file = data_dir / 'scopus_enriched.txt'
    if not scopus_file.exists():
        scopus_file = data_dir / 'scopus_converted_to_wos.txt'

    # å°è¯•æŸ¥æ‰¾æœ€ç»ˆæ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨æ¸…æ´—åçš„æ–‡ä»¶ï¼‰
    final_file = data_dir / 'Final_Version.txt'
    if not final_file.exists():
        final_file = data_dir / 'english_only.txt'

    # å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæ£€æŸ¥å¤±è´¥å°±ä¸åˆ›å»ºè¾“å‡ºç›®å½•ï¼‰
    print("\næ£€æŸ¥å¿…è¦æ–‡ä»¶...")
    missing_files = []
    if not wos_file.exists():
        missing_files.append(f"  âœ— WOS æ–‡ä»¶: {wos_file}")
    else:
        print(f"  âœ“ WOS æ–‡ä»¶: {wos_file}")

    if not scopus_file.exists():
        missing_files.append(f"  âœ— Scopus è½¬æ¢æ–‡ä»¶: {scopus_file}")
    else:
        print(f"  âœ“ Scopus è½¬æ¢æ–‡ä»¶: {scopus_file}")

    if not final_file.exists():
        missing_files.append(f"  âœ— æœ€ç»ˆæ•°æ®æ–‡ä»¶: {final_file}")
    else:
        print(f"  âœ“ æœ€ç»ˆæ•°æ®æ–‡ä»¶: {final_file}")

    if missing_files:
        print("\nâœ— ç¼ºå°‘å¿…è¦æ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨:")
        for msg in missing_files:
            print(msg)
        print("\næç¤ºï¼šè¯·ç¡®ä¿å·¥ä½œæµå·²å®Œæ•´æ‰§è¡Œ")
        return False

    print("âœ“ æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å­˜åœ¨\n")

    # æ–‡ä»¶éƒ½å­˜åœ¨ï¼Œåˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = data_dir / 'Figures and Tables' / '01 æ–‡æ¡£ç±»å‹'
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ“ è¾“å‡ºç›®å½•: {output_dir}\n")

    # æ˜¾ç¤ºå¹´ä»½ç­›é€‰ä¿¡æ¯
    if min_year or max_year:
        year_info = f"åº”ç”¨å¹´ä»½ç­›é€‰: {min_year or 'ä¸é™'} - {max_year or 'ä¸é™'}"
        print(f"âœ“ {year_info}")
    else:
        print("âš  æœªæŒ‡å®šå¹´ä»½èŒƒå›´ï¼ŒWOS å’Œ Scopus æ•°æ®å°†ä¸è¿›è¡Œå¹´ä»½ç­›é€‰")

    # åˆ†æ
    analyzer = DocumentTypeAnalyzer()
    data = analyzer.create_data_from_files(str(wos_file), str(scopus_file), str(final_file),
                                          min_year, max_year)

    # ä¿å­˜æ•°æ®
    print("æ­£åœ¨ä¿å­˜åˆ†æç»“æœ...")
    csv_file = output_dir / 'document_types_data.csv'
    data.to_csv(csv_file, index=False)
    print(f"  âœ“ ç»Ÿè®¡æ•°æ®: {csv_file.name}")

    # ç”Ÿæˆå›¾è¡¨
    print("æ­£åœ¨ç”Ÿæˆå›¾è¡¨...")
    analyzer.plot_distribution(data, str(output_dir))
    print(f"  âœ“ å›¾è¡¨æ–‡ä»¶: document_types.tiff å’Œ .png")

    # å¤åˆ¶æœ€ç»ˆæ•°æ®æ–‡ä»¶åˆ° data æ–‡ä»¶å¤¹ï¼ˆæ–¹ä¾¿ç›´æ¥ä½¿ç”¨ï¼‰
    print("æ­£åœ¨å¤åˆ¶æœ€ç»ˆæ•°æ®æ–‡ä»¶...")
    import shutil
    data_folder = data_dir / 'data'
    data_folder.mkdir(exist_ok=True)
    final_data_output = data_folder / 'download_final_data.txt'
    shutil.copy(final_file, final_data_output)
    print(f"  âœ“ åˆ†ææ•°æ®å·²å¤åˆ¶åˆ°: {final_data_output}")

    # ä¿å­˜ä»£ç å‰¯æœ¬
    code_copy = output_dir / 'plot_document_types.py'
    shutil.copy(__file__, code_copy)
    print(f"  âœ“ è„šæœ¬å‰¯æœ¬: {code_copy.name}")

    # æœ€åæ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š æ–‡æ¡£ç±»å‹åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print(f"\nå›¾è¡¨è¾“å‡ºç›®å½•: {output_dir}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  å›¾è¡¨æ–‡ä»¶å¤¹ ({output_dir.name}):")
    print(f"    - document_types.tiff           - é«˜æ¸…å›¾è¡¨ï¼ˆæŠ•ç¨¿ç”¨ï¼‰")
    print(f"    - document_types.png            - PNGå›¾è¡¨ï¼ˆé¢„è§ˆç”¨ï¼‰")
    print(f"    - document_types_data.csv       - ç»Ÿè®¡æ•°æ®ï¼ˆExcelå¯è¯»ï¼‰")
    print(f"    - plot_document_types.py        - ç»˜å›¾è„šæœ¬å‰¯æœ¬")
    print(f"\n  data æ–‡ä»¶å¤¹ ({data_dir / 'data'}):")
    print(f"    - download_final_data.txt       - ğŸ†• æœ€ç»ˆæ•°æ®ï¼ˆå¯ç›´æ¥ç”¨äºVOSviewer/CiteSpaceï¼‰")
    print("\nâœ“ å›¾è¡¨å·²ä¿å­˜åˆ°: {}".format(output_dir))
    print("âœ“ åˆ†ææ•°æ®å·²ä¿å­˜åˆ°: {}\n".format(data_dir / 'data' / 'download_final_data.txt'))

    return True


def generate_all_figures(data_dir: str, min_year: int = None, max_year: int = None):
    """ç”Ÿæˆæ‰€æœ‰å›¾è¡¨ï¼ˆæ–‡æ¡£ç±»å‹ + å¹´åº¦å‘æ–‡åŠå¼•ç”¨é‡ï¼‰

    Args:
        data_dir: æ•°æ®ç›®å½•
        min_year: æœ€å°å¹´ä»½ï¼ˆå¯é€‰ï¼‰
        max_year: æœ€å¤§å¹´ä»½ï¼ˆå¯é€‰ï¼‰

    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    print("\n" + "=" * 80)
    print("å¼€å§‹ç”Ÿæˆæ‰€æœ‰å›¾è¡¨...")
    print("=" * 80)

    success_count = 0
    total_count = 2

    # 1. ç”Ÿæˆæ–‡æ¡£ç±»å‹åˆ†æå›¾
    print("\n[1/2] æ–‡æ¡£ç±»å‹åˆ†æ")
    if generate_document_type_analysis(data_dir, min_year, max_year):
        success_count += 1

    # 2. ç”Ÿæˆå¹´åº¦å‘æ–‡åŠå¼•ç”¨é‡å›¾
    print("\n[2/2] å¹´åº¦å‘æ–‡åŠå¼•ç”¨é‡åˆ†æ")
    try:
        from .plot_citations import generate_publications_citations_analysis
        if generate_publications_citations_analysis(data_dir):
            success_count += 1
    except ImportError as e:
        print(f"âš  æ— æ³•å¯¼å…¥å¹´åº¦å‘æ–‡åŠå¼•ç”¨é‡åˆ†ææ¨¡å—: {e}")
    except Exception as e:
        print(f"âœ— å¹´åº¦å‘æ–‡åŠå¼•ç”¨é‡åˆ†æå¤±è´¥: {e}")

    # æ€»ç»“
    print("\n" + "=" * 80)
    print(f"å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/{total_count} ç»„å›¾è¡¨")
    print("=" * 80)

    if success_count == total_count:
        data_path = Path(data_dir)
        print("\nâœ“ æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆï¼š")
        print(f"\n  01 æ–‡æ¡£ç±»å‹ ({data_path / 'Figures and Tables' / '01 æ–‡æ¡£ç±»å‹'}):")
        print(f"    - document_types.tiff/png")
        print(f"    - document_types_data.csv")
        print(f"\n  02 å„å¹´å‘æ–‡åŠå¼•æ–‡é‡ ({data_path / 'Figures and Tables' / '02 å„å¹´å‘æ–‡åŠå¼•æ–‡é‡'}):")
        print(f"    - å„å¹´å‘æ–‡é‡.tiff/png")
        print(f"    - å„å¹´å¼•ç”¨é‡.tiff/png")
        print(f"    - å„å¹´å‘æ–‡é‡åŠå¼•ç”¨é‡.tiff/png")
        print(f"    - publications_citations_data.csv")
        print(f"\n  data æ–‡ä»¶å¤¹ ({data_path / 'data'}):")
        print(f"    - download_final_data.txt")
        print()

    return success_count == total_count


if __name__ == '__main__':
    import sys
    if len(sys.argv) > 1:
        # é»˜è®¤ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
        generate_all_figures(sys.argv[1])
    else:
        print("Usage: python3 plot_document_types.py <data_dir>")
