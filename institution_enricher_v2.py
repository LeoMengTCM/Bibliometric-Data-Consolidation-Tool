#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœºæ„ä¿¡æ¯è¡¥å…¨å™¨ v2.0ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

æ–°ç‰¹æ€§ï¼š
1. æ•°æ®åº“ç¼“å­˜ï¼ˆä¼˜å…ˆæŸ¥è¯¢æ•°æ®åº“ï¼Œæ²¡æœ‰æ‰è°ƒç”¨AIï¼‰
2. é‡è¯•æœºåˆ¶ï¼ˆAPIå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼‰
3. æ‰¹é‡å¤„ç†ä¼˜åŒ–ï¼ˆå®šæœŸä¿å­˜æ•°æ®åº“ï¼‰
4. å¢åŠ max_tokensåˆ°5000
5. è¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Š

ä½œè€…ï¼šMeng Linghan
å¼€å‘å·¥å…·ï¼šClaude Code
æ—¥æœŸï¼š2025-11-10
ç‰ˆæœ¬ï¼šv2.0
"""

import re
import json
import logging
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from gemini_config import GeminiConfig
from gemini_enricher_v2 import GeminiEnricherV2

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class InstitutionEnricherV2:
    """æœºæ„ä¿¡æ¯è¡¥å…¨å™¨v2.0ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""

    def __init__(self, gemini_config: GeminiConfig, db_path: str = 'config/institution_ai_cache.json'):
        """
        åˆå§‹åŒ–è¡¥å…¨å™¨

        Args:
            gemini_config: Gemini APIé…ç½®
            db_path: æ•°æ®åº“è·¯å¾„
        """
        self.gemini = GeminiEnricherV2(gemini_config, db_path)
        self.stats = {
            'total_processed': 0,
            'enriched': 0,
            'failed': 0,
            'db_hits': 0,
            'ai_calls': 0
        }

        logger.info("âœ“ æœºæ„ä¿¡æ¯è¡¥å…¨å™¨v2.0å·²åˆå§‹åŒ–")

    def enrich_c1_field(self, scopus_c1_text: str, authors: List[str]) -> Tuple[str, Dict]:
        """
        è¡¥å…¨æ•´ä¸ªC1å­—æ®µï¼ˆæ‰¹é‡å¤„ç†ï¼‰

        Args:
            scopus_c1_text: Scopusçš„C1å­—æ®µæ–‡æœ¬
            authors: ä½œè€…åˆ—è¡¨

        Returns:
            (è¡¥å…¨åçš„C1æ–‡æœ¬, ç»Ÿè®¡ä¿¡æ¯)
        """
        lines = [line.strip() for line in scopus_c1_text.split('\n') if line.strip()]

        # ç¬¬ä¸€æ­¥ï¼šè§£ææ‰€æœ‰è¡Œï¼Œæ”¶é›†éœ€è¦è¡¥å…¨çš„æœºæ„
        parsed_lines = []
        institutions_to_enrich = []

        # æ— æ•ˆæœºæ„å…³é”®è¯ï¼ˆè¿‡æ»¤æ‰å…¬å¸ã€Ltdç­‰ï¼‰
        invalid_keywords = ['Ltd.', 'Ltd', 'Inc.', 'Inc', 'Co.', 'Co', 'LLC', 'Corp.', 'Corp']

        for line in lines:
            parsed = self._parse_scopus_c1_line(line)
            parsed_lines.append((line, parsed))

            if parsed:
                # è¿‡æ»¤æ— æ•ˆæœºæ„
                inst_name = parsed['institution']
                if any(keyword in inst_name for keyword in invalid_keywords):
                    continue  # è·³è¿‡å…¬å¸åç§°

                inst_tuple = (inst_name, parsed['city'], parsed['country'])
                institutions_to_enrich.append(inst_tuple)

        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è¡¥å…¨æ‰€æœ‰æœºæ„ï¼ˆå»é‡ï¼‰
        unique_institutions = list(set(institutions_to_enrich))
        enrichment_results = self.gemini.enrich_institutions_batch(unique_institutions)

        # ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨è¡¥å…¨ç»“æœ
        enriched_lines = []
        line_stats = []

        for line, parsed in parsed_lines:
            if not parsed:
                enriched_lines.append(line)
                line_stats.append({'status': 'parse_failed', 'original': line})
                continue

            # è·å–è¡¥å…¨ç»“æœ
            inst_tuple = (parsed['institution'], parsed['city'], parsed['country'])
            enriched = enrichment_results.get(inst_tuple)

            if enriched and enriched['confidence'] > 0.7:
                # è¡¥å…¨æˆåŠŸï¼Œé‡æ–°æ„å»ºC1è¡Œ
                new_line = self._build_wos_c1_line(parsed['authors'], enriched)
                enriched_lines.append(new_line)
                line_stats.append({
                    'status': 'enriched',
                    'original': line,
                    'enriched': new_line,
                    'confidence': enriched['confidence']
                })
                self.stats['enriched'] += 1
            else:
                # è¡¥å…¨å¤±è´¥ï¼Œä¿æŒåŸæ ·
                enriched_lines.append(line)
                line_stats.append({'status': 'failed', 'original': line})
                self.stats['failed'] += 1

            self.stats['total_processed'] += 1

        enriched_c1 = '\n'.join(enriched_lines)
        return enriched_c1, {'lines': line_stats}

    def _parse_scopus_c1_line(self, c1_line: str) -> Optional[Dict]:
        """è§£æScopusçš„C1è¡Œ"""
        match = re.match(r'\[(.*?)\]\s+(.*)', c1_line)
        if not match:
            return None

        authors_str = match.group(1)
        address = match.group(2).rstrip('.')

        authors = [a.strip() for a in authors_str.split(';')]
        parts = [p.strip() for p in address.split(',')]

        if len(parts) < 2:
            return None

        country = parts[-1]
        city = parts[-2] if len(parts) >= 2 else None
        institution = parts[0]
        departments = parts[1:-2] if len(parts) > 2 else []

        return {
            'authors': authors,
            'institution': institution,
            'departments': departments,
            'city': city,
            'country': country
        }

    def _build_wos_c1_line(self, authors: List[str], enriched_info: Dict) -> str:
        """æ„å»ºWOSæ ¼å¼çš„C1è¡Œ"""
        authors_str = '; '.join(authors)
        address_parts = [enriched_info['institution_full_name']]

        if enriched_info.get('departments'):
            address_parts.extend(enriched_info['departments'])

        # æ„å»ºåœ°ç†ä¿¡æ¯éƒ¨åˆ†ï¼ˆç¡®ä¿countryæ€»æ˜¯å•ç‹¬çš„ä¸€éƒ¨åˆ†ï¼‰
        if enriched_info.get('state') and enriched_info.get('zip_code'):
            # ç¾å›½æ ¼å¼: City, State ZIP, Country
            # é‡è¦: Stateå’ŒZIPæ”¾åœ¨ä¸€èµ·ï¼Œä½†Countryå¿…é¡»å•ç‹¬ä½œä¸ºæœ€åä¸€éƒ¨åˆ†
            address_parts.append(enriched_info['city'])
            address_parts.append(f"{enriched_info['state']} {enriched_info['zip_code']}")
            address_parts.append(enriched_info['country'])
        elif enriched_info.get('zip_code'):
            # å…¶ä»–æ ¼å¼: City, ZIP, Country
            # é‡è¦: ZIPå•ç‹¬ä¸€éƒ¨åˆ†ï¼ŒCountryå•ç‹¬ä¸€éƒ¨åˆ†
            address_parts.append(enriched_info['city'])
            address_parts.append(enriched_info['zip_code'])
            address_parts.append(enriched_info['country'])
        elif enriched_info.get('state'):
            # åªæœ‰stateæ²¡æœ‰ZIP: City, State, Country
            address_parts.append(enriched_info['city'])
            address_parts.append(enriched_info['state'])
            address_parts.append(enriched_info['country'])
        else:
            # åªæœ‰åŸå¸‚å’Œå›½å®¶
            address_parts.append(enriched_info['city'])
            address_parts.append(enriched_info['country'])

        address_str = ', '.join(address_parts)
        c1_line = f"[{authors_str}] {address_str}."

        return c1_line

    def enrich_file(self, input_file: str, output_file: str, save_interval: int = 5) -> Dict:
        """
        è¡¥å…¨æ•´ä¸ªWOSæ–‡ä»¶çš„C1å­—æ®µ

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            save_interval: æ¯å¤„ç†å¤šå°‘æ¡è®°å½•ä¿å­˜ä¸€æ¬¡æ•°æ®åº“

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹è¡¥å…¨æ–‡ä»¶: {input_file}")

        # è¯»å–æ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8-sig') as f:
            content = f.read()

        # è§£æè®°å½•
        records = self._parse_wos_file(content)
        logger.info(f"è§£æäº† {len(records)} æ¡è®°å½•")

        # è¡¥å…¨æ¯æ¡è®°å½•çš„C1å­—æ®µ
        enriched_records = []
        for i, record in enumerate(records, 1):
            logger.info(f"å¤„ç†è®°å½• {i}/{len(records)}")

            if 'C1' in record and 'AU' in record:
                authors = [a.strip() for a in record['AU'].split('\n') if a.strip()]
                enriched_c1, line_stats = self.enrich_c1_field(record['C1'], authors)
                record['C1'] = enriched_c1

            enriched_records.append(record)

            # å®šæœŸä¿å­˜æ•°æ®åº“
            if i % save_interval == 0:
                self.gemini.db.save_database()
                logger.info(f"âœ“ å·²ä¿å­˜æ•°æ®åº“ï¼ˆè¿›åº¦: {i}/{len(records)}ï¼‰")

        # æœ€åä¿å­˜ä¸€æ¬¡æ•°æ®åº“
        self.gemini.db.save_database()

        # å†™å…¥æ–‡ä»¶
        self._write_wos_file(enriched_records, output_file)

        logger.info(f"è¡¥å…¨å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {output_file}")

        return self.get_statistics()

    def _parse_wos_file(self, content: str) -> List[Dict[str, str]]:
        """è§£æWOSæ–‡ä»¶"""
        records = []
        record_blocks = content.split('\n\nPT ')[1:]

        for block in record_blocks:
            block = 'PT ' + block
            record = self._parse_record(block)
            if record:
                records.append(record)

        return records

    def _parse_record(self, block: str) -> Dict[str, str]:
        """è§£æå•æ¡è®°å½•"""
        record = {}
        lines = block.split('\n')

        current_field = None
        current_value = []

        for line in lines:
            if line.strip() == 'ER':
                if current_field:
                    record[current_field] = '\n'.join(current_value)
                break

            if len(line) >= 3 and line[:2].isupper() and line[2] == ' ':
                if current_field:
                    record[current_field] = '\n'.join(current_value)
                current_field = line[:2]
                current_value = [line[3:]]
            elif line.startswith('   ') and current_field:
                current_value.append(line[3:])

        return record

    def _write_wos_file(self, records: List[Dict], output_file: str):
        """å†™å…¥WOSæ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8-sig') as f:
            f.write('FN Clarivate Analytics Web of Science\n')
            f.write('VR 1.0\n')

            for record in records:
                f.write('\nPT J\n')

                field_order = ['AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB',
                              'C1', 'C3', 'RP', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2',
                              'PU', 'SN', 'J9', 'JI', 'PY', 'VL', 'IS', 'AR', 'DI',
                              'WE', 'UT', 'PM', 'DA']

                for field in field_order:
                    if field in record:
                        value = record[field]
                        lines = value.split('\n')
                        f.write(f'{field} {lines[0]}\n')
                        for line in lines[1:]:
                            f.write(f'   {line}\n')

                f.write('ER\n')

            f.write('\nEF\n')

    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        gemini_stats = self.gemini.get_statistics()

        total = self.stats['total_processed']
        enrichment_rate = self.stats['enriched'] / total * 100 if total > 0 else 0

        return {
            'processing': {
                'total_processed': total,
                'enriched': self.stats['enriched'],
                'failed': self.stats['failed'],
                'enrichment_rate': f"{enrichment_rate:.1f}%"
            },
            'database': gemini_stats['database'],
            'session': gemini_stats['session']
        }

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_statistics()

        print("\n" + "=" * 80)
        print("æœºæ„ä¿¡æ¯è¡¥å…¨ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 80)
        print()
        print("ã€å¤„ç†ç»Ÿè®¡ã€‘")
        print(f"  æ€»å¤„ç†æ•°: {stats['processing']['total_processed']}")
        print(f"  è¡¥å…¨æˆåŠŸ: {stats['processing']['enriched']}")
        print(f"  è¡¥å…¨å¤±è´¥: {stats['processing']['failed']}")
        print(f"  è¡¥å…¨ç‡: {stats['processing']['enrichment_rate']}")
        print()
        print("ã€æ•°æ®åº“ç»Ÿè®¡ã€‘")
        print(f"  æ•°æ®åº“æ€»æœºæ„æ•°: {stats['database']['total_institutions']}")
        print(f"  å†å²AIè°ƒç”¨æ€»æ•°: {stats['database']['total_ai_calls_ever']}")
        print()
        print("ã€æœ¬æ¬¡ä¼šè¯ã€‘")
        print(f"  æ•°æ®åº“å‘½ä¸­: {stats['session']['db_hits']}")
        print(f"  æ•°æ®åº“æœªå‘½ä¸­: {stats['session']['db_misses']}")
        print(f"  å‘½ä¸­ç‡: {stats['database']['hit_rate']}")
        print(f"  AIè°ƒç”¨æ¬¡æ•°: {stats['session']['ai_calls']}")
        print()
        print("ğŸ’¡ æç¤º:")
        if stats['session']['ai_calls'] > 0:
            print(f"  - æœ¬æ¬¡æ–°å¢ {stats['session']['ai_calls']} ä¸ªæœºæ„åˆ°æ•°æ®åº“")
            print(f"  - ä¸‹æ¬¡è¿è¡Œè¿™äº›æœºæ„å°†ç›´æ¥ä»æ•°æ®åº“è¯»å–ï¼Œæ— éœ€è°ƒç”¨AI")
        if stats['session']['db_hits'] > 0:
            print(f"  - æœ¬æ¬¡ä»æ•°æ®åº“ç›´æ¥è·å–äº† {stats['session']['db_hits']} ä¸ªæœºæ„ä¿¡æ¯")
            print(f"  - èŠ‚çœäº† {stats['session']['db_hits']} æ¬¡AIè°ƒç”¨ï¼")
        print("=" * 80)


def main():
    """å‘½ä»¤è¡Œå·¥å…·"""
    import argparse

    parser = argparse.ArgumentParser(
        description='ä½¿ç”¨Gemini AIè¡¥å…¨æœºæ„ä¿¡æ¯ v2.0ï¼ˆä¼˜åŒ–ç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ–°ç‰¹æ€§:
  - æ•°æ®åº“ç¼“å­˜ï¼šä¼˜å…ˆæŸ¥è¯¢æ•°æ®åº“ï¼Œæ²¡æœ‰æ‰è°ƒç”¨AI
  - é‡è¯•æœºåˆ¶ï¼šAPIå¤±è´¥è‡ªåŠ¨é‡è¯•3æ¬¡
  - æ‰¹é‡ä¼˜åŒ–ï¼šå®šæœŸä¿å­˜æ•°æ®åº“
  - å¢åŠ tokenï¼šmax_tokenså¢åŠ åˆ°5000

ç¤ºä¾‹:
  # è¡¥å…¨å•ä¸ªæ–‡ä»¶
  python3 institution_enricher_v2.py --input scopus_converted.txt --output enriched.txt

  # ä½¿ç”¨è‡ªå®šä¹‰APIé…ç½®
  python3 institution_enricher_v2.py --input scopus_converted.txt --output enriched.txt \\
      --api-key YOUR_KEY --api-url YOUR_URL --model gemini-2.5-flash-lite
        """
    )

    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥WOSæ–‡ä»¶')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºWOSæ–‡ä»¶')
    parser.add_argument('--api-key', help='Gemini APIå¯†é’¥')
    parser.add_argument('--api-url', help='Gemini APIåœ°å€')
    parser.add_argument('--model', help='Geminiæ¨¡å‹åç§°')
    parser.add_argument('--db-path', default='config/institution_ai_cache.json',
                       help='æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤: config/institution_ai_cache.jsonï¼‰')
    parser.add_argument('--save-interval', type=int, default=5,
                       help='æ¯å¤„ç†å¤šå°‘æ¡è®°å½•ä¿å­˜ä¸€æ¬¡æ•°æ®åº“ï¼ˆé»˜è®¤: 5ï¼‰')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       default='INFO', help='æ—¥å¿—çº§åˆ«')

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.getLogger().setLevel(getattr(logging, args.log_level))

    # åˆ›å»ºGeminié…ç½®
    if args.api_key:
        config = GeminiConfig.from_params(
            api_key=args.api_key,
            api_url=args.api_url or 'https://gptload.drmeng.top/proxy/bibliometrics/v1beta',
            model=args.model or 'gemini-2.5-flash-lite'
        )
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = GeminiConfig.from_params(
            api_key='sk-leomeng1997',
            api_url='https://gptload.drmeng.top/proxy/bibliometrics/v1beta',
            model='gemini-2.5-flash-lite'
        )

    if not config.validate():
        print("âœ— Gemini APIé…ç½®æ— æ•ˆ")
        return

    print("\n" + "=" * 80)
    print("æœºæ„ä¿¡æ¯è¡¥å…¨å™¨ v2.0ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    print("=" * 80)
    print(f"è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"æ•°æ®åº“: {args.db_path}")
    print(f"APIæ¨¡å‹: {config.model}")
    print(f"Max tokens: {config.max_tokens}")
    print(f"é‡è¯•æ¬¡æ•°: {config.max_retries}")
    print("=" * 80)
    print()

    # åˆ›å»ºè¡¥å…¨å™¨
    enricher = InstitutionEnricherV2(config, args.db_path)

    # è¡¥å…¨æ–‡ä»¶
    stats = enricher.enrich_file(args.input, args.output, args.save_interval)

    # æ‰“å°ç»Ÿè®¡
    enricher.print_statistics()


if __name__ == '__main__':
    main()
